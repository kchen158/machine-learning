{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading collection 'popular'\n",
      "[nltk_data]    | \n",
      "[nltk_data]    | Downloading package cmudict to\n",
      "[nltk_data]    |     /Users/ferd/nltk_data...\n",
      "[nltk_data]    |   Package cmudict is already up-to-date!\n",
      "[nltk_data]    | Downloading package gazetteers to\n",
      "[nltk_data]    |     /Users/ferd/nltk_data...\n",
      "[nltk_data]    |   Package gazetteers is already up-to-date!\n",
      "[nltk_data]    | Downloading package genesis to\n",
      "[nltk_data]    |     /Users/ferd/nltk_data...\n",
      "[nltk_data]    |   Package genesis is already up-to-date!\n",
      "[nltk_data]    | Downloading package gutenberg to\n",
      "[nltk_data]    |     /Users/ferd/nltk_data...\n",
      "[nltk_data]    |   Package gutenberg is already up-to-date!\n",
      "[nltk_data]    | Downloading package inaugural to\n",
      "[nltk_data]    |     /Users/ferd/nltk_data...\n",
      "[nltk_data]    |   Package inaugural is already up-to-date!\n",
      "[nltk_data]    | Downloading package movie_reviews to\n",
      "[nltk_data]    |     /Users/ferd/nltk_data...\n",
      "[nltk_data]    |   Package movie_reviews is already up-to-date!\n",
      "[nltk_data]    | Downloading package names to /Users/ferd/nltk_data...\n",
      "[nltk_data]    |   Package names is already up-to-date!\n",
      "[nltk_data]    | Downloading package shakespeare to\n",
      "[nltk_data]    |     /Users/ferd/nltk_data...\n",
      "[nltk_data]    |   Package shakespeare is already up-to-date!\n",
      "[nltk_data]    | Downloading package stopwords to\n",
      "[nltk_data]    |     /Users/ferd/nltk_data...\n",
      "[nltk_data]    |   Package stopwords is already up-to-date!\n",
      "[nltk_data]    | Downloading package treebank to\n",
      "[nltk_data]    |     /Users/ferd/nltk_data...\n",
      "[nltk_data]    |   Package treebank is already up-to-date!\n",
      "[nltk_data]    | Downloading package twitter_samples to\n",
      "[nltk_data]    |     /Users/ferd/nltk_data...\n",
      "[nltk_data]    |   Package twitter_samples is already up-to-date!\n",
      "[nltk_data]    | Downloading package omw to /Users/ferd/nltk_data...\n",
      "[nltk_data]    |   Package omw is already up-to-date!\n",
      "[nltk_data]    | Downloading package wordnet to\n",
      "[nltk_data]    |     /Users/ferd/nltk_data...\n",
      "[nltk_data]    |   Package wordnet is already up-to-date!\n",
      "[nltk_data]    | Downloading package wordnet_ic to\n",
      "[nltk_data]    |     /Users/ferd/nltk_data...\n",
      "[nltk_data]    |   Package wordnet_ic is already up-to-date!\n",
      "[nltk_data]    | Downloading package words to /Users/ferd/nltk_data...\n",
      "[nltk_data]    |   Package words is already up-to-date!\n",
      "[nltk_data]    | Downloading package maxent_ne_chunker to\n",
      "[nltk_data]    |     /Users/ferd/nltk_data...\n",
      "[nltk_data]    |   Package maxent_ne_chunker is already up-to-date!\n",
      "[nltk_data]    | Downloading package punkt to /Users/ferd/nltk_data...\n",
      "[nltk_data]    |   Package punkt is already up-to-date!\n",
      "[nltk_data]    | Downloading package snowball_data to\n",
      "[nltk_data]    |     /Users/ferd/nltk_data...\n",
      "[nltk_data]    |   Package snowball_data is already up-to-date!\n",
      "[nltk_data]    | Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]    |     /Users/ferd/nltk_data...\n",
      "[nltk_data]    |   Package averaged_perceptron_tagger is already up-\n",
      "[nltk_data]    |       to-date!\n",
      "[nltk_data]    | \n",
      "[nltk_data]  Done downloading collection popular\n"
     ]
    }
   ],
   "source": [
    "import glob\n",
    "import string\n",
    "import numpy as np\n",
    "from pandas import DataFrame\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn import metrics\n",
    "from collections import Counter\n",
    "\n",
    "import nltk\n",
    "nltk.download('popular')\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "directory = [('enronTrain/ham','ham'),('enronTrain/spam','spam')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainHamFiles = glob.glob('enronTrain/ham/*.txt')\n",
    "trainSpamFiles = glob.glob('enronTrain/spam/*.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Helper Functions - Pre-Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Remove punctuations from txt file\n",
    "def removePunc(emailText):\n",
    "    puncResult = ''.join(x for x in emailText if x not in string.punctuation)\n",
    "    return puncResult\n",
    "\n",
    "#Remove captial letters from the txt file\n",
    "def removeCap(emailText):\n",
    "    lowerResult = emailText.lower()\n",
    "    return lowerResult\n",
    "\n",
    "#Remove common stop words using nltk stop word list\n",
    "def removeStop(emailText):\n",
    "    stop_list = stopwords.words('english')\n",
    "    stop_list.append('subject')\n",
    "\n",
    "    tokens = word_tokenize(emailText)\n",
    "    stopResult = ' '.join(y for y in tokens if not y in stop_list)\n",
    "    return stopResult\n",
    "\n",
    "#Find the most common owrds of given corpus\n",
    "def mostCommonWords(contentDf,num):\n",
    "    sumContents = contentDf.content.str.cat(sep=' ')\n",
    "    tokens = word_tokenize(sumContents)\n",
    "    frequency_dist = nltk.FreqDist(tokens)\n",
    "    return frequency_dist.most_common(num)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Helper Function - Data Collection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Read the contents of all txt files of a given path \n",
    "def readFile(path,noStop):\n",
    "    trainFiles = glob.glob(path+'/*.txt')\n",
    "    \n",
    "    for file in trainFiles:\n",
    "        fileObject = open(file,'rb')\n",
    "        fileContents = fileObject.read().decode('utf-8','ignore')\n",
    "        fileContents = fileContents.replace('\\n', ' ')\n",
    "        fileContents = removePunc(fileContents)\n",
    "        fileContents = removeCap(fileContents)\n",
    "        if noStop is True:\n",
    "            fileContents = removeStop(fileContents)\n",
    "        fileObject.close()\n",
    "        yield fileContents\n",
    "\n",
    "#Create a pandas data frame of the email contnets with labels\n",
    "def createDataFrame(path,label,noStop):\n",
    "    rows = []\n",
    "    \n",
    "    for fileContent in readFile(path,noStop):\n",
    "        rows.append({'content':fileContent, 'label':label})\n",
    "    df = DataFrame(rows)\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train Model: No Capitilization, No punctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Build data frame of all the training emails enron1-enron5\n",
    "trainData1 = DataFrame({'content':[],'label':[]})\n",
    "for path, label in directory:\n",
    "    trainData1 = trainData1.append(createDataFrame(path,label,False))\n",
    "    \n",
    "#Create a bag of words representation based on word frequency of training data\n",
    "trainVectorizer1 = CountVectorizer()\n",
    "bagOfWordsTrain1 = trainVectorizer1.fit_transform(trainData1.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.7/site-packages/sklearn/naive_bayes.py:485: UserWarning: alpha too small will result in numeric errors, setting alpha = 1.0e-10\n",
      "  'setting alpha = %.1e' % _ALPHA_MIN)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "MultinomialNB(alpha=0, class_prior=None, fit_prior=True)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Fit Naive Bayes model\n",
    "clf1 = MultinomialNB()\n",
    "clf2 = MultinomialNB(alpha=0)\n",
    "clf1.fit(bagOfWordsTrain1,trainData1.label)\n",
    "clf2.fit(bagOfWordsTrain1,trainData1.label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create a data frame to extract features of test data\n",
    "testDirectory = [('enronTest/ham','ham'),('enronTest/spam','spam')]\n",
    "testData1 = DataFrame({'content':[],'label':[]})\n",
    "\n",
    "for path, label in testDirectory:\n",
    "    testData1 = testData1.append(createDataFrame(path,label,False))\n",
    "    \n",
    "bagOfWordsTest1 = trainVectorizer1.transform(testData1.content)\n",
    "resultsWithStopLap = clf1.predict(bagOfWordsTest1)\n",
    "resultsWithStopNoLap = clf2.predict(bagOfWordsTest1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy With Stop Words & Laplace Smoothing:  0.983\n",
      "Accuracy With Stop Words & No Lapalace Smoothing:  0.9791666666666666\n"
     ]
    }
   ],
   "source": [
    "print('Accuracy With Stop Words & Laplace Smoothing: ', metrics.accuracy_score(testData1.label,resultsWithStopLap))\n",
    "print('Accuracy With Stop Words & No Lapalace Smoothing: ', metrics.accuracy_score(testData1.label,resultsWithStopNoLap))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Most Frequent Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 10 Most Frequent Ham Words With Stop Words:\n",
      "[('the', 166481), ('to', 119021), ('and', 76114), ('of', 70479), ('a', 60482), ('in', 55667), ('enron', 55334), ('for', 45435), ('on', 38392), ('i', 35588)]\n",
      "\n",
      "Top 10 Most Frequent Spam Words With Stop Words:\n",
      "[('the', 74749), ('to', 59967), ('and', 51802), ('of', 47514), ('a', 38100), ('you', 35064), ('in', 31755), ('your', 25821), ('for', 25593), ('this', 23851)]\n"
     ]
    }
   ],
   "source": [
    "#Find the most common 10 words form ham and spam emails\n",
    "hamTrainContent1 = trainData1.iloc[:15045,:]\n",
    "spamTrainContent1 = trainData1.iloc[15045:,:]\n",
    "\n",
    "print('Top 10 Most Frequent Ham Words With Stop Words:')\n",
    "print(mostCommonWords(hamTrainContent1,10),end='\\n\\n')\n",
    "print('Top 10 Most Frequent Spam Words With Stop Words:')\n",
    "print(mostCommonWords(spamTrainContent1,10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Most Discriminative Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "logProb = clf1.feature_log_prob_\n",
    "\n",
    "discrimHam = logProb[0,:] - logProb[1,:]\n",
    "discrimSpam = logProb[1,:] - logProb[0,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 5 Most Discriminative Ham Words: ['enron' 'kaminski' 'dynegy' 'ect' 'ees' 'ena' 'dbcaps' 'hourahead'\n",
      " 'fastow' 'mmbtu']\n",
      "Top 5 Most Discriminative Spam Words ['pills' 'viagra' 'computron' 'cialis' 'nbsp' 'photoshop' 'width' 'href'\n",
      " 'voip' 'paypal']\n"
     ]
    }
   ],
   "source": [
    "indexMaxHam = np.argsort(discrimHam)[::-1]\n",
    "indexMaxSpam = np.argsort(discrimSpam)[::-1]\n",
    "\n",
    "topHam = np.array(trainVectorizer1.get_feature_names())[indexMaxHam[0:10]]\n",
    "topSpam = np.array(trainVectorizer1.get_feature_names())[indexMaxSpam[0:10]]\n",
    "\n",
    "print('Top 5 Most Discriminative Ham Words:',topHam)\n",
    "print('Top 5 Most Discriminative Spam Words',topSpam)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train Model: No Capitilization, No Punctuation, No Stop Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Build data frame of all the training emails enron1-enron5\n",
    "trainData2 = DataFrame({'content':[],'label':[]})\n",
    "for path, label in directory:\n",
    "    trainData2 = trainData2.append(createDataFrame(path,label,True))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "136031\n"
     ]
    }
   ],
   "source": [
    "print(len(trainVectorizer2.vocabulary_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create a bag of words representation based on word frequency of training data\n",
    "trainVectorizer2 = CountVectorizer()\n",
    "bagOfWordsTrain2 = trainVectorizer2.fit_transform(trainData2.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.7/site-packages/sklearn/naive_bayes.py:485: UserWarning: alpha too small will result in numeric errors, setting alpha = 1.0e-10\n",
      "  'setting alpha = %.1e' % _ALPHA_MIN)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "MultinomialNB(alpha=0, class_prior=None, fit_prior=True)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Fit Naive Bayes model\n",
    "clf3 = MultinomialNB()\n",
    "clf4 = MultinomialNB(alpha=0)\n",
    "clf3.fit(bagOfWordsTrain2,trainData2.label)\n",
    "clf4.fit(bagOfWordsTrain2,trainData2.label)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Most Frequent Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 10 Most Frequent Ham Words Without Stop Words:\n",
      "[('enron', 55334), ('ect', 35223), ('hou', 16893), ('2001', 14700), ('2000', 12851), ('1', 12638), ('please', 11867), ('would', 11833), ('company', 11321), ('com', 11265)]\n",
      "\n",
      "Top 10 Most Frequent Spam Words Without Stop Words:\n",
      "[('com', 8842), ('1', 7794), ('3', 7661), ('company', 6938), ('2', 6793), ('http', 6729), ('e', 6647), ('email', 6001), ('information', 5492), ('5', 5388)]\n"
     ]
    }
   ],
   "source": [
    "# Get the top 10 most common words for ham and spam emails without stop words\n",
    "#Find the most common 10 words form ham and spam emails\n",
    "hamTrainContent2 = trainData2.iloc[:15045,:]\n",
    "spamTrainContent2 = trainData2.iloc[15045:,:]\n",
    "\n",
    "print('Top 10 Most Frequent Ham Words Without Stop Words:')\n",
    "print(mostCommonWords(hamTrainContent2,10),end='\\n\\n')\n",
    "print('Top 10 Most Frequent Spam Words Without Stop Words:')\n",
    "print(mostCommonWords(spamTrainContent2,10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Most Discriminative Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "logProb = clf3.feature_log_prob_\n",
    "\n",
    "discrimHam = logProb[0,:] - logProb[1,:]\n",
    "discrimSpam = logProb[1,:] - logProb[0,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 5 Most Discriminative Ham Words: ['enron' 'kaminski' 'dynegy' 'ect' 'ees' 'ena' 'dbcaps' 'hourahead'\n",
      " 'fastow' 'mmbtu']\n",
      "Top 5 Most Discriminative Spam Words ['pills' 'viagra' 'computron' 'cialis' 'nbsp' 'photoshop' 'width' 'href'\n",
      " 'voip' 'paypal']\n"
     ]
    }
   ],
   "source": [
    "indexMaxHam = np.argsort(discrimHam)[::-1]\n",
    "indexMaxSpam = np.argsort(discrimSpam)[::-1]\n",
    "\n",
    "topHam = np.array(trainVectorizer2.get_feature_names())[indexMaxHam[0:10]]\n",
    "topSpam = np.array(trainVectorizer2.get_feature_names())[indexMaxSpam[0:10]]\n",
    "\n",
    "print('Top 5 Most Discriminative Ham Words:',topHam)\n",
    "print('Top 5 Most Discriminative Spam Words',topSpam)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Create a data frame to extract features of test data\n",
    "testDirectory = [('enronTest/ham','ham'),('enronTest/spam','spam')]\n",
    "testData2 = DataFrame({'content':[],'label':[]})\n",
    "\n",
    "for path, label in testDirectory:\n",
    "    testData2 = testData2.append(createDataFrame(path,label,True))\n",
    "    \n",
    "bagOfWordsTest2 = trainVectorizer2.transform(testData2.content)\n",
    "resultsNoStopLap = clf3.predict(bagOfWordsTest2)\n",
    "resultsNoStopNoLap = clf4.predict(bagOfWordsTest2)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy No Stop Words & Laplace Smoothing:  0.9821666666666666\n",
      "Accuracy No Stop Words & No Lapalace Smoothing:  0.9781666666666666\n"
     ]
    }
   ],
   "source": [
    "print('Accuracy No Stop Words & Laplace Smoothing: ', metrics.accuracy_score(testData2.label,resultsNoStopLap))\n",
    "print('Accuracy No Stop Words & No Lapalace Smoothing: ', metrics.accuracy_score(testData2.label,resultsNoStopNoLap))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
